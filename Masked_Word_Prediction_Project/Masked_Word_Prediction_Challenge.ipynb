{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Masked Word Prediction Challenge\n",
        "# Authors: Lindy Bujak & Melia [Last Name if you want]\n",
        "# Course: DATASCI 315 - Machine Learning in Python\n",
        "# (Private) Kaggle Competition: https://www.kaggle.com/t/579b97152fa0478698f9574c589539a8"
      ],
      "metadata": {
        "id": "MOQUvr3Psy19"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import random\n",
        "\n",
        "from collections import Counter # to build vocab easily"
      ],
      "metadata": {
        "id": "Dk45SVmcR_U2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7dUjsoy9PWtc"
      },
      "outputs": [],
      "source": [
        "# Load text file\n",
        "def load_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read().lower()\n",
        "    return text\n",
        "\n",
        "train_data = load_text(\"data/train_data.txt\")\n",
        "test_data = load_text(\"data/test_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and Vocabulary - assuming everything is in the vocab from train # note: no option for pad or unknown token\n",
        "def build_vocab(text, min_freq=1):\n",
        "    tokens = text.split()\n",
        "    counter = Counter(tokens)\n",
        "    vocab = {word: idx for idx, (word, count) in enumerate(counter.items(), start=2) if count >= min_freq}\n",
        "    vocab[\"<mask>\"] = 0\n",
        "    vocab[\"<unk>\"] = 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_data)"
      ],
      "metadata": {
        "id": "A-bH05EOSPxd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "class Tokenizer:\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.inv_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = text.lower().replace('.', ' .') # to ensure <mask>. can be spit\n",
        "        special_tokens = {\"<mask>\"}  # Ensure special tokens are not split\n",
        "        tokens = text.split()  # Use simple split() to preserve \"<mask>\" as a single token\n",
        "        return [self.vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join([self.inv_vocab.get(token, \"<unk>\") for token in tokens])\n",
        "\n",
        "tokenizer = Tokenizer(vocab)"
      ],
      "metadata": {
        "id": "_OeMGpZcTWLc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTInspiredTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=256, num_layers=6, num_heads=8,\n",
        "                 intermediate_size=1024, dropout=0.1, max_position_embeddings=512):\n",
        "        self.setting = {\n",
        "            'name': 'BERTInspiredTransformer',\n",
        "            'vocab_size': vocab_size,\n",
        "            'hidden_dim': hidden_dim,\n",
        "            'num_layers': num_layers,\n",
        "            'num_heads': num_heads,\n",
        "            'intermediate_size': intermediate_size,\n",
        "            'dropout': dropout,\n",
        "            'max_position_embeddings': max_position_embeddings\n",
        "        }\n",
        "        super(BERTInspiredTransformer, self).__init__()\n",
        "\n",
        "        # Word embeddings\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, hidden_dim)\n",
        "\n",
        "        # Position embeddings (learnable instead of fixed)\n",
        "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_dim)\n",
        "\n",
        "        # Layer normalization for embeddings\n",
        "        self.LayerNorm = nn.LayerNorm(hidden_dim, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Register position ids buffer\n",
        "        position_ids = torch.arange(max_position_embeddings).expand((1, -1))\n",
        "        self.register_buffer('position_ids', position_ids)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=intermediate_size,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True  # Pre-LN architecture\n",
        "        )\n",
        "\n",
        "        # Encoder with layer normalization\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            norm=nn.LayerNorm(hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Special MLM prediction head - with transformation similar to BERT\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, vocab_size)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\" Initialize the weights based on BERT initialization pattern \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly larger range for small vocabulary\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = self.position_ids[:, :seq_length]\n",
        "\n",
        "        # Get embeddings\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # Sum embeddings\n",
        "        embeddings = words_embeddings + position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        encoder_output = self.encoder(embeddings)\n",
        "\n",
        "        # Apply prediction head for masked language modeling\n",
        "        prediction_scores = self.prediction_head(encoder_output)\n",
        "\n",
        "        return prediction_scores"
      ],
      "metadata": {
        "id": "7yVoMAxLnH3B"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocusedMaskedDataset(Dataset):\n",
        "    def __init__(self, text, vocab, tokenizer, seq_length=32, mask_prob=0.15, random_seed=42):\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "        self.seq_length = seq_length\n",
        "        self.mask_prob = mask_prob\n",
        "        self.mask_token_id = vocab[\"<mask>\"]\n",
        "\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx\n",
        "        tokens = self.tokens[start_idx:start_idx+self.seq_length]\n",
        "        masked_tokens, mask, actual_tokens = self.mask_tokens(tokens)\n",
        "        return torch.tensor(masked_tokens), torch.tensor(mask), torch.tensor(actual_tokens)\n",
        "\n",
        "    def mask_tokens(self, tokens):\n",
        "        masked_tokens = tokens.copy()\n",
        "        mask = [0] * len(tokens)\n",
        "        actual_tokens = tokens.copy()\n",
        "\n",
        "        # Ensure at least one token is masked (80% of the time)\n",
        "        if random.random() < 0.8:\n",
        "            # Choose a position to definitely mask\n",
        "            pos = random.randint(0, len(tokens) - 1)\n",
        "            masked_tokens[pos] = self.mask_token_id\n",
        "            mask[pos] = 1\n",
        "\n",
        "        # Then apply regular masking with probability\n",
        "        for i in range(len(tokens)):\n",
        "            if mask[i] == 0 and random.random() < self.mask_prob:\n",
        "                # BERT-style masking:\n",
        "                # 80% of the time, replace with [MASK]\n",
        "                # 10% of the time, replace with random word\n",
        "                # 10% of the time, keep the word unchanged\n",
        "                rand = random.random()\n",
        "                if rand < 0.8:\n",
        "                    masked_tokens[i] = self.mask_token_id\n",
        "                elif rand < 0.9:\n",
        "                    # Replace with random word\n",
        "                    masked_tokens[i] = random.randint(2, len(self.vocab) - 1)  # Skip special tokens\n",
        "                # else: keep the token unchanged\n",
        "                mask[i] = 1\n",
        "\n",
        "        return masked_tokens, mask, actual_tokens\n",
        "\n",
        "# Modified training function with learning rate warmup and focus on masked tokens\n",
        "def specialized_train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=15):\n",
        "    model.to(device)\n",
        "    best_val_acc = 0\n",
        "    best_model = None\n",
        "\n",
        "    # For tracking metrics\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for masked_tokens, mask, actual_tokens in train_loader:\n",
        "            masked_tokens, mask, actual_tokens = masked_tokens.to(device), mask.to(device), actual_tokens.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(masked_tokens)\n",
        "\n",
        "            # Calculate loss only on masked tokens\n",
        "            masked_output = output.view(-1, output.size(-1))\n",
        "            masked_targets = actual_tokens.view(-1)\n",
        "            masked_loss = criterion(masked_output, masked_targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            masked_loss.backward()\n",
        "\n",
        "            # Apply gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += masked_loss.item()\n",
        "\n",
        "            # Calculate accuracy only on masked positions\n",
        "            predicted_tokens = output.argmax(dim=-1)\n",
        "            correct_train += ((predicted_tokens == actual_tokens) & (mask == 1)).sum().item()\n",
        "            total_train += mask.sum().item()\n",
        "\n",
        "        train_accuracy = (correct_train / total_train) * 100 if total_train > 0 else 0\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_accs.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for masked_tokens, mask, actual_tokens in val_loader:\n",
        "                masked_tokens, mask, actual_tokens = masked_tokens.to(device), mask.to(device), actual_tokens.to(device)\n",
        "\n",
        "                output = model(masked_tokens)\n",
        "\n",
        "                # Loss on masked tokens only\n",
        "                masked_output = output.view(-1, output.size(-1))\n",
        "                masked_targets = actual_tokens.view(-1)\n",
        "                masked_loss = criterion(masked_output, masked_targets)\n",
        "\n",
        "                total_val_loss += masked_loss.item()\n",
        "\n",
        "                # Calculate accuracy only on masked positions\n",
        "                predicted_tokens = output.argmax(dim=-1)\n",
        "                correct_val += ((predicted_tokens == actual_tokens) & (mask == 1)).sum().item()\n",
        "                total_val += mask.sum().item()\n",
        "\n",
        "        val_accuracy = (correct_val / total_val) * 100 if total_val > 0 else 0\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            best_model = model.state_dict().copy()\n",
        "            print(f\"  New best model saved with validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Load best model\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "        print(f\"Loaded best model with validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return model, {\n",
        "        'train_loss': train_losses,\n",
        "        'train_acc': train_accs,\n",
        "        'val_loss': val_losses,\n",
        "        'val_acc': val_accs\n",
        "    }\n",
        "\n",
        "# Training setup\n",
        "def prepare_and_train():\n",
        "    # Load data\n",
        "    train_data = load_text(\"/content/train_data.txt\")\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocab = build_vocab(train_data)\n",
        "    tokenizer = Tokenizer(vocab)\n",
        "    vocab_size = len(vocab)\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    # Create dataset with longer sequence length\n",
        "    seq_length = 32  # Longer sequences for more context\n",
        "    dataset = FocusedMaskedDataset(train_data, vocab, tokenizer, seq_length=seq_length, mask_prob=0.15)\n",
        "\n",
        "    # Split dataset with fixed seed for reproducibility\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    print(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n",
        "\n",
        "    model = BERTInspiredTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=128,\n",
        "        num_layers=4,\n",
        "        num_heads=8,\n",
        "        intermediate_size=512,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-4,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    # Loss function with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model, history = specialized_train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=15 ###########\n",
        "    )\n",
        "\n",
        "    # Save the model\n",
        "    torch.save({\n",
        "        'state_dict': model.state_dict(),\n",
        "        'setting': model.setting\n",
        "    }, \"/content/bert_inspired_transformer.pth\")\n",
        "\n",
        "    return model, tokenizer, vocab, history"
      ],
      "metadata": {
        "id": "9XMTPQyInOi2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specialized prediction function for BERT-inspired model\n",
        "def predict_with_bert_model(model, text, vocab, tokenizer, seq_length, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized = tokenizer.encode(text)\n",
        "    mask_token_id = vocab[\"<mask>\"]\n",
        "\n",
        "    # Find mask positions\n",
        "    mask_positions = [i for i, token in enumerate(tokenized) if token == mask_token_id]\n",
        "    predicted_words = [\"\"] * len(mask_positions)\n",
        "\n",
        "    # For each mask position, create context window\n",
        "    for i, pos in enumerate(mask_positions):\n",
        "        # Create window centered on mask\n",
        "        half_len = seq_length // 2\n",
        "        start = max(0, pos - half_len)\n",
        "        end = min(len(tokenized), start + seq_length)\n",
        "\n",
        "        # Adjust window if at the edges\n",
        "        if end - start < seq_length:\n",
        "            if start == 0:\n",
        "                end = min(len(tokenized), seq_length)\n",
        "            else:\n",
        "                start = max(0, end - seq_length)\n",
        "\n",
        "        # Extract the window\n",
        "        window = tokenized[start:end]\n",
        "\n",
        "        # Calculate mask position in window\n",
        "        window_mask_pos = pos - start\n",
        "\n",
        "        # Convert to tensor\n",
        "        input_tensor = torch.tensor([window], device=device)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor)\n",
        "\n",
        "            # Get prediction for masked position\n",
        "            if window_mask_pos < len(window):\n",
        "                logits = outputs[0, window_mask_pos]\n",
        "\n",
        "                # Get top prediction\n",
        "                pred_id = torch.argmax(logits).item()\n",
        "                pred_word = tokenizer.inv_vocab.get(pred_id, \"<unk>\")\n",
        "\n",
        "                # Store prediction\n",
        "                predicted_words[i] = pred_word\n",
        "\n",
        "    # Check predictions\n",
        "    missing = sum(1 for word in predicted_words if not word)\n",
        "    if missing > 0:\n",
        "        print(f\"Warning: {missing} mask positions have no predictions.\")\n",
        "\n",
        "        # Fill missing predictions\n",
        "        for i in range(len(predicted_words)):\n",
        "            if not predicted_words[i]:\n",
        "                predicted_words[i] = \"the\"  # Fallback to common word\n",
        "\n",
        "    return predicted_words\n",
        "\n",
        "# Full prediction and submission function\n",
        "def generate_predictions_and_submit():\n",
        "    # Load the trained model\n",
        "    model_data = torch.load(\"/content/bert_inspired_transformer.pth\")\n",
        "\n",
        "    # Reload data and vocabulary\n",
        "    train_data = load_text(\"/content/train_data.txt\")\n",
        "    test_data = load_text(\"/content/test_data.txt\")\n",
        "    vocab = build_vocab(train_data)\n",
        "    tokenizer = Tokenizer(vocab)\n",
        "\n",
        "    # Create model with the same architecture\n",
        "    model = BERTInspiredTransformer(\n",
        "        vocab_size=len(vocab),\n",
        "        hidden_dim=128,\n",
        "        num_layers=4,\n",
        "        num_heads=8,\n",
        "        intermediate_size=512,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # Load state dictionary\n",
        "    model.load_state_dict(model_data['state_dict'])\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Generate predictions\n",
        "    predicted_words = predict_with_bert_model(\n",
        "        model=model,\n",
        "        text=test_data,\n",
        "        vocab=vocab,\n",
        "        tokenizer=tokenizer,\n",
        "        seq_length=32,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Check predictions\n",
        "    print(f\"Total predictions: {len(predicted_words)}\")\n",
        "    print(f\"First 10 predictions: {predicted_words[:10]}\")\n",
        "    print(f\"Last 10 predictions: {predicted_words[-10:]}\")\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    df = pd.DataFrame({\n",
        "        'id': range(len(predicted_words)),\n",
        "        'prediction': predicted_words\n",
        "    })\n",
        "\n",
        "    # Save to CSV\n",
        "    csv_path = \"/content/bert_predictions.csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Predictions saved to {csv_path}\")\n",
        "\n",
        "    return predicted_words\n",
        "\n",
        "# Run everything\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "\n",
        "    # Train model\n",
        "    print(\"================= TRAINING MODEL =================\")\n",
        "    model, tokenizer, vocab, history = prepare_and_train()\n",
        "\n",
        "    # Generate predictions\n",
        "    print(\"\\n=============== GENERATING PREDICTIONS ===============\")\n",
        "    predictions = generate_predictions_and_submit()"
      ],
      "metadata": {
        "id": "UJzdZ4HgnTGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be8e305-05b2-4785-d143-a9ae0d9a8ccc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================= TRAINING MODEL =================\n",
            "Vocabulary size: 22\n",
            "Training samples: 96921, Validation samples: 10770\n",
            "Using device: cuda\n",
            "Epoch 1/15, Train Loss: 0.8410, Train Acc: 48.21%, Val Loss: 0.7634, Val Acc: 58.11%\n",
            "  New best model saved with validation accuracy: 58.11%\n",
            "Epoch 2/15, Train Loss: 0.7647, Train Acc: 58.31%, Val Loss: 0.7564, Val Acc: 60.01%\n",
            "  New best model saved with validation accuracy: 60.01%\n",
            "Epoch 3/15, Train Loss: 0.7588, Train Acc: 59.25%, Val Loss: 0.7525, Val Acc: 60.01%\n",
            "  New best model saved with validation accuracy: 60.01%\n",
            "Epoch 4/15, Train Loss: 0.7556, Train Acc: 59.79%, Val Loss: 0.7497, Val Acc: 60.85%\n",
            "  New best model saved with validation accuracy: 60.85%\n",
            "Epoch 5/15, Train Loss: 0.7534, Train Acc: 60.07%, Val Loss: 0.7489, Val Acc: 61.23%\n",
            "  New best model saved with validation accuracy: 61.23%\n",
            "Epoch 6/15, Train Loss: 0.7523, Train Acc: 60.40%, Val Loss: 0.7478, Val Acc: 61.32%\n",
            "  New best model saved with validation accuracy: 61.32%\n",
            "Epoch 7/15, Train Loss: 0.7513, Train Acc: 60.54%, Val Loss: 0.7476, Val Acc: 61.21%\n",
            "Epoch 8/15, Train Loss: 0.7509, Train Acc: 60.59%, Val Loss: 0.7482, Val Acc: 61.00%\n",
            "Epoch 9/15, Train Loss: 0.7506, Train Acc: 60.72%, Val Loss: 0.7476, Val Acc: 61.31%\n",
            "Epoch 10/15, Train Loss: 0.7500, Train Acc: 60.79%, Val Loss: 0.7474, Val Acc: 61.17%\n",
            "Epoch 11/15, Train Loss: 0.7496, Train Acc: 60.81%, Val Loss: 0.7466, Val Acc: 61.43%\n",
            "  New best model saved with validation accuracy: 61.43%\n",
            "Epoch 12/15, Train Loss: 0.7494, Train Acc: 60.89%, Val Loss: 0.7464, Val Acc: 61.41%\n",
            "Epoch 13/15, Train Loss: 0.7487, Train Acc: 61.07%, Val Loss: 0.7461, Val Acc: 61.45%\n",
            "  New best model saved with validation accuracy: 61.45%\n",
            "Epoch 14/15, Train Loss: 0.7484, Train Acc: 61.14%, Val Loss: 0.7459, Val Acc: 61.80%\n",
            "  New best model saved with validation accuracy: 61.80%\n",
            "Epoch 15/15, Train Loss: 0.7484, Train Acc: 61.13%, Val Loss: 0.7460, Val Acc: 61.84%\n",
            "  New best model saved with validation accuracy: 61.84%\n",
            "Loaded best model with validation accuracy: 61.84%\n",
            "\n",
            "=============== GENERATING PREDICTIONS ===============\n",
            "Using device: cuda\n",
            "Total predictions: 30000\n",
            "First 10 predictions: ['big', '.', 'car', 'car', 'big', 'bird', 'this', '.', 'cat', 'a']\n",
            "Last 10 predictions: ['park', '.', 'dog', 'big', 'in', '.', 'the', 'the', 'dog', '.']\n",
            "Predictions saved to /content/bert_predictions.csv\n"
          ]
        }
      ]
    }
  ]
}